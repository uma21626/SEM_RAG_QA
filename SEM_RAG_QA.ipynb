{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0007f7-687c-4417-a091-cc18be13e429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uma21\\anaconda3\\envs\\rag_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Imports & Model Setup\n",
    "# ===============================\n",
    "import subprocess\n",
    "import nltk\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer\n",
    "import pdfplumber\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "TAU_E = 0.40   # entity threshold\n",
    "TAU_D = 0.35   # chunk threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511b58ab-9745-4443-ba40-4ef8c3a4de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, text, page):\n",
    "        self.text = text\n",
    "        self.page = page\n",
    "        self.entities = []\n",
    "        self.embedding = None\n",
    "        self.community = None\n",
    "        self.entity_relations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca49544-dd32-4009-a732-f3f6047845d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Load PDF & Sentence Extraction\n",
    "# ===============================\n",
    "def load_pdf(pdf_path: str):\n",
    "    \"\"\"Load PDF with pdfplumber\"\"\"\n",
    "    pages = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                pages.append((i + 1, text))           \n",
    "    return pages\n",
    "\n",
    "def extract_sentences(pages):\n",
    "    \"\"\"Extract sentences with page numbers\"\"\"\n",
    "    sentences = []\n",
    "    for page_num, text in pages:\n",
    "        for sent in sent_tokenize(text):\n",
    "            sentences.append({\n",
    "                \"text\": sent,\n",
    "                \"page\": page_num\n",
    "            })\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80005063-9f34-4483-a60c-6ee4c6eeb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Semantic Chunking \n",
    "# ===============================\n",
    "\n",
    "def semantic_chunking(sentences, sim_threshold=0.75):\n",
    "    \"\"\"Semantic chunking via cosine similarity\"\"\"\n",
    "    texts = [s[\"text\"] for s in sentences]\n",
    "    embeddings = embedder.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "    chunks = []\n",
    "    buffer = [sentences[0]]\n",
    "    buffer_embedding = embeddings[0]\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity(\n",
    "            [buffer_embedding],\n",
    "            [embeddings[i]]\n",
    "        )[0][0]\n",
    "\n",
    "        if sim >= sim_threshold:\n",
    "            buffer.append(sentences[i])\n",
    "            # Update buffer embedding as average\n",
    "            buffer_indices = [j for j in range(i - len(buffer) + 1, i + 1)]\n",
    "            buffer_embedding = np.mean(embeddings[buffer_indices], axis=0)\n",
    "        else:\n",
    "            # Create chunk from buffer\n",
    "            chunks.append(buffer)\n",
    "            buffer = [sentences[i]]\n",
    "            buffer_embedding = embeddings[i]\n",
    "\n",
    "    chunks.append(buffer)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4717aa-0a77-497f-9301-1e693645e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Buffer Merge & Token Control\n",
    "# ===============================\n",
    "def buffer_merge(chunks, max_tokens=1024, subchunk_size=128):\n",
    "    \"\"\"Merge chunks respecting token limits\"\"\"\n",
    "    merged = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = \" \".join(s[\"text\"] for s in chunk)\n",
    "        page = chunk[0][\"page\"] if chunk else 1\n",
    "        \n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            merged.append(Chunk(text, page))\n",
    "        else:\n",
    "            # Create sub-chunks with overlap\n",
    "            for i in range(0, len(tokens), subchunk_size):\n",
    "                end_idx = min(i + subchunk_size, len(tokens))\n",
    "                sub_tokens = tokens[i:end_idx]\n",
    "                \n",
    "                # Add overlap\n",
    "                if i > 0:\n",
    "                    overlap_start = max(0, i - 32)\n",
    "                    overlap_tokens = tokens[overlap_start:i]\n",
    "                    sub_tokens = overlap_tokens + sub_tokens\n",
    "                \n",
    "                sub_text = tokenizer.decode(sub_tokens)\n",
    "                merged.append(Chunk(sub_text, page))\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a3aeb1-7798-4380-ba27-bc19ea2f65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Entity & Embeddings\n",
    "# ===============================\n",
    "def embed_chunks(chunks):\n",
    "    \"\"\"Embed chunks using sentence transformer\"\"\"\n",
    "    for chunk in chunks:\n",
    "        chunk.embedding = embedder.encode(chunk.text, normalize_embeddings=True)\n",
    "    return chunks\n",
    "def enrich_chunks(chunks):\n",
    "    \"\"\"Extract entities and relations from chunks\"\"\"\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk.text)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'NORP', 'EVENT', 'WORK_OF_ART']:\n",
    "                entities.append({\n",
    "                    'text': ent.text,\n",
    "                    'label': ent.label_,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char\n",
    "                })\n",
    "        chunk.entities = [e['text'] for e in entities]\n",
    "        \n",
    "        # Extract simple relations (subject-verb-object)\n",
    "        relations = []\n",
    "        for sent in doc.sents:\n",
    "            # Find subject, verb, object patterns\n",
    "            for token in sent:\n",
    "                if token.dep_ in ['nsubj', 'nsubjpass']:\n",
    "                    subj = token.text\n",
    "                    # Find the root verb\n",
    "                    root = token.head\n",
    "                    while root.head != root:\n",
    "                        root = root.head\n",
    "                    \n",
    "                    # Find objects\n",
    "                    objs = [child.text for child in root.children \n",
    "                           if child.dep_ in ['dobj', 'pobj', 'attr']]\n",
    "                    \n",
    "                    for obj in objs:\n",
    "                        if subj in chunk.entities and obj in chunk.entities:\n",
    "                            relations.append({\n",
    "                                'subject': subj,\n",
    "                                'relation': root.lemma_,\n",
    "                                'object': obj\n",
    "                            })\n",
    "        \n",
    "        chunk.entity_relations = relations\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bc548f-a7cc-4d4d-b9d5-95a4ea2a4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Knowledge Graph\n",
    "# ===============================\n",
    "def build_knowledge_graph(chunks):\n",
    "    \"\"\"Build knowledge graph with entities and relationships\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes (entities)\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        for entity in chunk.entities:\n",
    "            if not G.has_node(entity):\n",
    "                G.add_node(entity, \n",
    "                          chunks=[idx], \n",
    "                          frequency=1,\n",
    "                          type='entity')\n",
    "            else:\n",
    "                G.nodes[entity]['chunks'].append(idx)\n",
    "                G.nodes[entity]['frequency'] += 1\n",
    "    \n",
    "    # Add edges based on co-occurrence and relations\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        # Co-occurrence edges\n",
    "        entities_in_chunk = list(set(chunk.entities))\n",
    "        for i in range(len(entities_in_chunk)):\n",
    "            for j in range(i + 1, len(entities_in_chunk)):\n",
    "                e1, e2 = entities_in_chunk[i], entities_in_chunk[j]\n",
    "                \n",
    "                if not G.has_edge(e1, e2):\n",
    "                    G.add_edge(e1, e2, \n",
    "                              weight=1,\n",
    "                              relations=[],\n",
    "                              chunks=[idx])\n",
    "                else:\n",
    "                    G[e1][e2]['weight'] += 1\n",
    "                    if idx not in G[e1][e2]['chunks']:\n",
    "                        G[e1][e2]['chunks'].append(idx)\n",
    "        \n",
    "        # Relation-based edges\n",
    "        for rel in chunk.entity_relations:\n",
    "            if G.has_node(rel['subject']) and G.has_node(rel['object']):\n",
    "                if not G.has_edge(rel['subject'], rel['object']):\n",
    "                    G.add_edge(rel['subject'], rel['object'],\n",
    "                              weight=2,  # Higher weight for explicit relations\n",
    "                              relations=[rel['relation']],\n",
    "                              chunks=[idx])\n",
    "                else:\n",
    "                    G[rel['subject']][rel['object']]['weight'] += 2\n",
    "                    if rel['relation'] not in G[rel['subject']][rel['object']]['relations']:\n",
    "                        G[rel['subject']][rel['object']]['relations'].append(rel['relation'])\n",
    "    \n",
    "    print(f\"Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293fd10a-61a2-4e7a-8de8-9ae15d4f8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Community Detection\n",
    "# ===============================\n",
    "def detect_communities(G, chunks):\n",
    "    \"\"\"Detect communities using Louvain algorithm\"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(\"Empty graph, no communities to detect\")\n",
    "        return chunks, {}\n",
    "    \n",
    "    # Create weighted graph for community detection\n",
    "    weighted_graph = G.copy()\n",
    "    for u, v, data in weighted_graph.edges(data=True):\n",
    "        if 'weight' not in data:\n",
    "            data['weight'] = 1\n",
    "    \n",
    "    try:\n",
    "        partition = community_louvain.best_partition(weighted_graph, weight='weight')\n",
    "        \n",
    "        # Map communities to chunks\n",
    "        community_chunks = defaultdict(set)\n",
    "        for entity, comm in partition.items():\n",
    "            if entity in G.nodes:\n",
    "                for chunk_id in G.nodes[entity]['chunks']:\n",
    "                    community_chunks[chunk_id].add(comm)\n",
    "        \n",
    "        # Assign dominant community to each chunk\n",
    "        for chunk_id, communities in community_chunks.items():\n",
    "            if chunk_id < len(chunks):\n",
    "                # Get the most frequent community\n",
    "                if communities:\n",
    "                    from collections import Counter\n",
    "                    comm_counter = Counter(communities)\n",
    "                    dominant_comm = comm_counter.most_common(1)[0][0]\n",
    "                    chunks[chunk_id].community = dominant_comm\n",
    "        \n",
    "        print(f\"Detected {len(set(partition.values()))} communities\")\n",
    "        return chunks, partition\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Community detection failed: {e}\")\n",
    "        return chunks, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853f4976-e894-48e0-b4d8-8ba0abb57c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_graph_rag_search(query, chunks, top_k=5):\n",
    "    \"\"\"Equation 4: Local Graph RAG Search\"\"\"\n",
    "    q_emb = embedder.encode(query, normalize_embeddings=True)\n",
    "    results = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk.embedding is None:\n",
    "            continue\n",
    "            \n",
    "        # Chunk similarity\n",
    "        chunk_sim = cosine_similarity([q_emb], [chunk.embedding])[0][0]\n",
    "        \n",
    "        # Entity relevance\n",
    "        entity_relevance = 0\n",
    "        if chunk.entities:\n",
    "            entity_embeddings = embedder.encode(chunk.entities, normalize_embeddings=True)\n",
    "            entity_sims = cosine_similarity([q_emb], entity_embeddings)[0]\n",
    "            entity_relevance = np.max(entity_sims) if len(entity_sims) > 0 else 0\n",
    "        \n",
    "        # Combined score: α * chunk_sim + (1-α) * entity_relevance\n",
    "        alpha = 0.7\n",
    "        combined_score = alpha * chunk_sim + (1 - alpha) * entity_relevance\n",
    "        \n",
    "        if combined_score > TAU_D:\n",
    "            results.append((chunk, combined_score, chunk_sim, entity_relevance))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0ac0dc-24f2-43b0-a25d-8f0ea3a9ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_graph_rag_search(query, chunks, top_k=3):\n",
    "    \"\"\"Equation 5: Global Graph RAG Search\"\"\"\n",
    "    q_emb = embedder.encode(query, normalize_embeddings=True)\n",
    "    \n",
    "    # Group chunks by community\n",
    "    comm_chunks = defaultdict(list)\n",
    "    for chunk in chunks:\n",
    "        if chunk.community is not None:\n",
    "            comm_chunks[chunk.community].append(chunk)\n",
    "    \n",
    "    # Calculate community embeddings and scores\n",
    "    comm_scores = {}\n",
    "    for comm, comm_chunks_list in comm_chunks.items():\n",
    "        if not comm_chunks_list:\n",
    "            continue\n",
    "            \n",
    "        # Community embedding as average of chunk embeddings\n",
    "        chunk_embeddings = [c.embedding for c in comm_chunks_list if c.embedding is not None]\n",
    "        if not chunk_embeddings:\n",
    "            continue\n",
    "            \n",
    "        comm_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "        \n",
    "        # Community similarity\n",
    "        comm_sim = cosine_similarity([q_emb], [comm_embedding])[0][0]\n",
    "        \n",
    "        # Entity relevance in community\n",
    "        all_entities = []\n",
    "        for chunk in comm_chunks_list:\n",
    "            all_entities.extend(chunk.entities)\n",
    "        \n",
    "        entity_relevance = 0\n",
    "        if all_entities:\n",
    "            unique_entities = list(set(all_entities))\n",
    "            entity_embeddings = embedder.encode(unique_entities[:10], normalize_embeddings=True)\n",
    "            entity_sims = cosine_similarity([q_emb], entity_embeddings)[0]\n",
    "            entity_relevance = np.mean(entity_sims) if len(entity_sims) > 0 else 0\n",
    "        \n",
    "        # Global score: β * comm_sim + γ * entity_relevance\n",
    "        beta, gamma = 0.6, 0.3\n",
    "        global_score = beta * comm_sim + gamma * entity_relevance\n",
    "        comm_scores[comm] = (global_score, comm_sim, entity_relevance)\n",
    "    \n",
    "    # Sort communities by score\n",
    "    ranked_comms = sorted(comm_scores.items(), key=lambda x: x[1][0], reverse=True)[:top_k]\n",
    "    \n",
    "    # Get chunks from top communities\n",
    "    top_chunks = []\n",
    "    for comm, (score, comm_sim, entity_rel) in ranked_comms:\n",
    "        for chunk in comm_chunks[comm]:\n",
    "            top_chunks.append((chunk, score, comm_sim, entity_rel))\n",
    "    \n",
    "    # Sort chunks by score\n",
    "    top_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "    return top_chunks[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a6d736-cfbc-486c-93e1-ca541f1c0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_llm(prompt, model=\"llama3.2:1b\"):\n",
    "    \"\"\"Generate response using Ollama\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model],\n",
    "            input=prompt.encode(),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=30\n",
    "        )\n",
    "        return result.stdout.decode().strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Ollama request timed out.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83905a74-d79f-482f-854d-32e4681739b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM Integration & Answer Generation\n",
    "# ============================================================\n",
    "def generate_answer(query, local_docs, global_docs):\n",
    "    \"\"\"Generate answer using retrieved context\"\"\"\n",
    "    \n",
    "    # Combine local and global contexts\n",
    "    all_docs = []\n",
    "    if local_docs:\n",
    "        all_docs.extend([(c, s) for c, s, _, _ in local_docs])\n",
    "    if global_docs:\n",
    "        all_docs.extend([(c, s) for c, s, _, _ in global_docs])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_docs = {}\n",
    "    for chunk, score in all_docs:\n",
    "        if chunk.text not in unique_docs:\n",
    "            unique_docs[chunk.text] = (chunk, score)\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_docs = sorted(unique_docs.values(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    # Prepare context\n",
    "    context_parts = []\n",
    "    for i, (chunk, score) in enumerate(sorted_docs, 1):\n",
    "        context_parts.append(f\"[Context {i}, Score: {score:.3f}, Page: {chunk.page}]\\n{chunk.text[:800]}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert assistant answering questions based on Dr. B.R. Ambedkar's writings.\n",
    "\n",
    "CONTEXT FROM AMBEDKAR'S WRITINGS:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer using ONLY the provided context above\n",
    "2. If the context doesn't contain relevant information, say so\n",
    "3. Be accurate and concise\n",
    "4. Reference specific parts of the context when possible\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    return ollama_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c6b447-5510-4a15-9284-5cc1a47d9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# System Initialization Pipeline\n",
    "# ============================================================\n",
    "def initialize_semrag():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SEMRAG System \")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load and process PDF\n",
    "    print(\"\\n1. Loading PDF...\")\n",
    "    pages = load_pdf(\"Ambedkar_book.pdf\")\n",
    "    print(f\"   Loaded {len(pages)} pages\")\n",
    "    \n",
    "    # 2. Extract sentences\n",
    "    print(\"\\n2. Extracting sentences...\")\n",
    "    sentences = extract_sentences(pages)\n",
    "    print(f\"   Extracted {len(sentences)} sentences\")\n",
    "    \n",
    "    # 3. Semantic chunking\n",
    "    print(\"\\n3. Performing semantic chunking...\")\n",
    "    sem_chunks = semantic_chunking(sentences[:2000])  # Limit for testing\n",
    "    print(f\"   Created {len(sem_chunks)} semantic groups\")\n",
    "    \n",
    "    # 4. Buffer merging\n",
    "    print(\"\\n4. Merging chunks with token limits...\")\n",
    "    chunks = buffer_merge(sem_chunks, max_tokens=1024, subchunk_size=128)\n",
    "    print(f\"   Created {len(chunks)} final chunks\")\n",
    "    \n",
    "    # 5. Enrich chunks with entities and embeddings\n",
    "    print(\"\\n5. Enriching chunks with entities and embeddings...\")\n",
    "    chunks = enrich_chunks(chunks)\n",
    "    chunks = embed_chunks(chunks)\n",
    "    \n",
    "    # 6. Build knowledge graph\n",
    "    print(\"\\n6. Building knowledge graph...\")\n",
    "    G = build_knowledge_graph(chunks)\n",
    "    \n",
    "    # 7. Detect communities\n",
    "    print(\"\\n7. Detecting communities...\")\n",
    "    chunks, partition = detect_communities(G, chunks)\n",
    "\n",
    "    return chunks, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab1e60fd-0a51-40bc-9598-de96b549257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SEMRAG System \n",
      "============================================================\n",
      "\n",
      "1. Loading PDF...\n",
      "   Loaded 92 pages\n",
      "\n",
      "2. Extracting sentences...\n",
      "   Extracted 2198 sentences\n",
      "\n",
      "3. Performing semantic chunking...\n",
      "   Created 1955 semantic groups\n",
      "\n",
      "4. Merging chunks with token limits...\n",
      "   Created 1955 final chunks\n",
      "\n",
      "5. Enriching chunks with entities and embeddings...\n",
      "\n",
      "6. Building knowledge graph...\n",
      "Graph built with 438 nodes and 800 edges\n",
      "\n",
      "7. Detecting communities...\n",
      "Detected 108 communities\n"
     ]
    }
   ],
   "source": [
    "chunks, G = initialize_semrag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87d2622-e738-45c4-8641-cae366ca1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Question Answering Interface\n",
    "# ============================================================\n",
    "def semrag_qa(query, chunks):        \n",
    "        # Local search\n",
    "        local_docs = local_graph_rag_search(query, chunks)\n",
    "        print(f\"Local chunks retrieved: {len(local_docs)}\")\n",
    "        # Global search\n",
    "        global_docs = global_graph_rag_search(query, chunks)\n",
    "        print(f\"Global communities retrieved: {len(global_docs)}\")\n",
    "        # Fallback if local search fails\n",
    "        if not local_docs and global_docs:\n",
    "            print(\" Falling back to global retrieval\")\n",
    "            local_docs = [(c, 0.0) for c in global_docs[:5]]\n",
    "    \n",
    "        if not local_docs:\n",
    "            return \"No relevant information found in the document.\", [], []\n",
    "            \n",
    "        # Generate answer\n",
    "        answer = generate_answer(query, local_docs, global_docs)\n",
    "        \n",
    "        return answer, local_docs, global_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2ab4b3-a754-4b0d-8140-be80e5cfc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was Ambedkar's view on caste discrimination?\n",
      "Local chunks retrieved: 5\n",
      "Global communities retrieved: 3\n",
      "Answer: Based on the provided context, Dr. B.R. Ambedkar's view on caste discrimination is that it is an inherent characteristic of Hinduism and the Indian caste system. He criticizes Mr. Nesfield for dwelling on this aspect as one of its characteristics. Additionally, Ambedkar suggests that his views are vindicated by Mahatma Gandhi's article \"A Vindication of Caste\".\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SEMRAG Question Answering - 1\n",
    "# ===============================\n",
    "query = \"What was Ambedkar's view on caste discrimination?\"\n",
    "print('Question:' , query)\n",
    "\n",
    "answer, local_docs, global_docs = semrag_qa(query, chunks)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d396eebb-b48b-4587-88e6-f657d7f53abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are Ambedkar's views on quantum mechanics?\n",
      "Local chunks retrieved: 5\n",
      "Global communities retrieved: 3\n",
      "Answer: Based on the provided context, it appears that Dr. B.R. Ambedkar has expressed his views on quantum mechanics, but these views are not explicitly stated. However, the context does mention that he is a challenge to Hinduism and is its most uncompromising exponent.\n",
      "\n",
      "It can be inferred that Ambedkar's views on quantum mechanics may not align with traditional Hindu perspectives, given that he is challenging the dominant Hindu worldview. Unfortunately, no specific information is provided in the context about his views on quantum mechanics.\n",
      "\n",
      "Therefore, I will not be able to provide a definitive answer based on the available text.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SEMRAG Question Answering - 2\n",
    "# ===============================\n",
    "query = \"What are Ambedkar's views on quantum mechanics?\"\n",
    "print('Question:' , query)\n",
    "\n",
    "answer, local_docs, global_docs = semrag_qa(query, chunks)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c17e4c5-ee91-4b16-b758-df7288685850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What were Ambedkar's views on Mahatma Gandhi's assassination?\n",
      "Local chunks retrieved: 5\n",
      "Global communities retrieved: 3\n",
      "Answer: Based on the provided context, it appears that Dr. B.R. Ambedkar's views on Mahatma Gandhi's assassination are not explicitly stated in the given passage. The context only mentions that Dr. Ambedkar was to have presided over a conference of the Jat-Pat-Todak Mandal of Lahore and appreciated Mahatma Gandhi for taking notice of his speech on Caste, which he had prepared.\n",
      "\n",
      "However, it is likely that Dr. Ambedkar's views on this topic would be mentioned in other contexts or writings, but these specific passages do not provide any information about Gandhi's assassination.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SEMRAG Question Answering - 3\n",
    "# ===============================\n",
    "query = \"What were Ambedkar's views on Mahatma Gandhi's assassination?\"\n",
    "print('Question:' , query)\n",
    "\n",
    "answer, local_docs, global_docs = semrag_qa(query, chunks)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4a2aa9b-3469-42ff-8f76-474c7dd2186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Ambedkar say about time travel?\n",
      "Local chunks retrieved: 5\n",
      "Global communities retrieved: 3\n",
      "Answer: Based on the provided context, Dr. B.R. Ambedkar did not mention time travel. In fact, he addressed a question about Hinduism and its scriptures, specifically referring to the Vedas, Upanishads, Smritis, Puranas, including Ramayana and Mahabharata, as the Hindu Scriptures.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SEMRAG Question Answering - 4\n",
    "# ===============================\n",
    "query = \"What did Ambedkar say about time travel?\"\n",
    "print('Question:' , query)\n",
    "\n",
    "answer, local_docs, global_docs = semrag_qa(query, chunks)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f697fa6d-ca8c-4428-9932-bf921f6a2083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Ambedkar say about social justice and equality?\n",
      "Local chunks retrieved: 5\n",
      "Global communities retrieved: 3\n",
      "Answer: Based on the provided context, Dr. Ambedkar stated that he was an opponent to Hinduism and its caste system. He specifically mentioned being \"oppressed among the Hindus\" (Context 2).\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SEMRAG Question Answering - 5\n",
    "# ===============================\n",
    "query =\"What did Ambedkar say about social justice and equality?\"\n",
    "print('Question:' , query)\n",
    "\n",
    "answer, local_docs, global_docs = semrag_qa(query, chunks)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a00bd03c-7241-4304-b98b-534c5a1d9e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did Ambedkar say about labor and workers' rights?\n",
      "Local chunks retrieved: 5\n",
      "Global communities retrieved: 3\n",
      "Answer: Based on the provided context from Dr. B.R. Ambedkar's writings, it is clear that he stated that laborers are \"untouchables\" in Hindu society.\n",
      "\n",
      "From Context 3, which states: \"How then can they object to Dr. Ambedkar’s address merely because he said that that was his last speech as a Hindu ?\"\n",
      "\n",
      "Context 4 also explicitly mentions the caste system and the Vedas, Upanishads, Smritis and Puranas including Ramayana and Mahabharata, which further reinforces this statement.\n",
      "\n",
      "It is worth noting that Ambedkar's critique of Hinduism's social hierarchy was based on his experiences as a Dalit (untouchable) in Indian society.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SEMRAG Question Answering - 6\n",
    "# ===============================\n",
    "query =\"What did Ambedkar say about labor and workers' rights?\"\n",
    "print('Question:' , query)\n",
    "\n",
    "answer, local_docs, global_docs = semrag_qa(query, chunks)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db3bb2-3289-4539-b729-da011dce72f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
